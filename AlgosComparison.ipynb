{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCo8Pn5xxloW0Y8+/gsBGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qubit19/AlgosInCuda/blob/main/AlgosComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4_jkceUrsFx",
        "outputId": "18ba65d7-bcea-48ff-9fce-18e39892c447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,607 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,701 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,863 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,154 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [47.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Fetched 18.3 MB in 4s (4,327 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compiler-11-8\n",
            "  cuda-cudart-11-8 cuda-cudart-dev-11-8 cuda-cuobjdump-11-8 cuda-cupti-11-8\n",
            "  cuda-cupti-dev-11-8 cuda-cuxxfilt-11-8 cuda-documentation-11-8\n",
            "  cuda-driver-dev-11-8 cuda-gdb-11-8 cuda-libraries-11-8\n",
            "  cuda-libraries-dev-11-8 cuda-memcheck-11-8 cuda-nsight-11-8\n",
            "  cuda-nsight-compute-11-8 cuda-nsight-systems-11-8 cuda-nvcc-11-8\n",
            "  cuda-nvdisasm-11-8 cuda-nvml-dev-11-8 cuda-nvprof-11-8 cuda-nvprune-11-8\n",
            "  cuda-nvrtc-11-8 cuda-nvrtc-dev-11-8 cuda-nvtx-11-8 cuda-nvvp-11-8\n",
            "  cuda-profiler-api-11-8 cuda-sanitizer-11-8 cuda-toolkit-11-8-config-common\n",
            "  cuda-toolkit-11-config-common cuda-tools-11-8 cuda-visual-tools-11-8\n",
            "  default-jre default-jre-headless fonts-dejavu-core fonts-dejavu-extra\n",
            "  gds-tools-11-8 libatk-wrapper-java libatk-wrapper-java-jni libcublas-11-8\n",
            "  libcublas-dev-11-8 libcufft-11-8 libcufft-dev-11-8 libcufile-11-8\n",
            "  libcufile-dev-11-8 libcurand-11-8 libcurand-dev-11-8 libcusolver-11-8\n",
            "  libcusolver-dev-11-8 libcusparse-11-8 libcusparse-dev-11-8 libnpp-11-8\n",
            "  libnpp-dev-11-8 libnvjpeg-11-8 libnvjpeg-dev-11-8 libtinfo5 libxcb-icccm4\n",
            "  libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1\n",
            "  libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 libxtst6\n",
            "  libxxf86dga1 nsight-compute-2022.3.0 nsight-systems-2022.4.2 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compiler-11-8\n",
            "  cuda-cudart-11-8 cuda-cudart-dev-11-8 cuda-cuobjdump-11-8 cuda-cupti-11-8\n",
            "  cuda-cupti-dev-11-8 cuda-cuxxfilt-11-8 cuda-documentation-11-8\n",
            "  cuda-driver-dev-11-8 cuda-gdb-11-8 cuda-libraries-11-8\n",
            "  cuda-libraries-dev-11-8 cuda-memcheck-11-8 cuda-nsight-11-8\n",
            "  cuda-nsight-compute-11-8 cuda-nsight-systems-11-8 cuda-nvcc-11-8\n",
            "  cuda-nvdisasm-11-8 cuda-nvml-dev-11-8 cuda-nvprof-11-8 cuda-nvprune-11-8\n",
            "  cuda-nvrtc-11-8 cuda-nvrtc-dev-11-8 cuda-nvtx-11-8 cuda-nvvp-11-8\n",
            "  cuda-profiler-api-11-8 cuda-sanitizer-11-8 cuda-toolkit-11-8\n",
            "  cuda-toolkit-11-8-config-common cuda-toolkit-11-config-common\n",
            "  cuda-tools-11-8 cuda-visual-tools-11-8 default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gds-tools-11-8 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-11-8 libcublas-dev-11-8 libcufft-11-8\n",
            "  libcufft-dev-11-8 libcufile-11-8 libcufile-dev-11-8 libcurand-11-8\n",
            "  libcurand-dev-11-8 libcusolver-11-8 libcusolver-dev-11-8 libcusparse-11-8\n",
            "  libcusparse-dev-11-8 libnpp-11-8 libnpp-dev-11-8 libnvjpeg-11-8\n",
            "  libnvjpeg-dev-11-8 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 libxtst6 libxxf86dga1 nsight-compute-2022.3.0\n",
            "  nsight-systems-2022.4.2 openjdk-11-jre x11-utils\n",
            "0 upgraded, 73 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 2,715 MB of archives.\n",
            "After this operation, 6,662 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.26+4-1ubuntu1~22.04 [214 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-11-8 11.8.89-1 [1,040 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-11-8 11.8.87-1 [15.4 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-11-8 11.8.87-1 [2,552 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-11-8 11.8.86-1 [50.8 MB]\n",
            "Get:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-11-8 11.8.86-1 [165 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-11-8 11.8.86-1 [4,138 kB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-memcheck-11-8 11.8.86-1 [142 kB]\n",
            "Get:28 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-11-8 11.8.87-1 [1,959 kB]\n",
            "Get:29 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-11-8 11.8.86-1 [51.3 kB]\n",
            "Get:30 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-11-8 11.8.86-1 [8,784 kB]\n",
            "Get:31 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-11-8 11.8.0-1 [2,472 B]\n",
            "Get:32 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-11-8 11.8.86-1 [189 kB]\n",
            "Get:33 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-config-common 11.8.89-1 [16.4 kB]\n",
            "Get:34 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-8-config-common 11.8.89-1 [16.3 kB]\n",
            "Get:35 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-11-8 11.8.89-1 [165 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-11-8 11.8.89-1 [27.3 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-11-8 11.8.89-1 [820 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-11-8 11.8.89-1 [43.5 MB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-11-8 11.8.86-1 [58.1 kB]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-11-8 11.8.0-1 [2,432 B]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-11-8 11.8.86-1 [49.8 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-11-8 11.8.89-1 [16.4 MB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-11-8 11.11.3.6-1 [248 MB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-11-8 10.9.0.58-1 [94.2 MB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-11-8 1.4.0.31-1 [474 kB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-11-8 10.3.0.86-1 [42.2 MB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-11-8 11.4.1.48-1 [52.3 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-11-8 11.7.5.86-1 [116 MB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-11-8 11.8.0.86-1 [102 MB]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-11-8 11.9.0.86-1 [1,865 kB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-11-8 11.8.0-1 [2,518 B]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-11-8 11.8.86-1 [18.5 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-11-8 11.8.89-1 [13.5 MB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-11-8 11.11.3.6-1 [269 MB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-11-8 10.9.0.58-1 [189 MB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-11-8 1.4.0.31-1 [1,062 kB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-11-8 10.3.0.86-1 [42.9 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-11-8 11.4.1.48-1 [35.7 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-11-8 11.7.5.86-1 [116 MB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-11-8 11.8.0.86-1 [100 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-11-8 11.9.0.86-1 [1,536 kB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-11-8 11.8.0-1 [2,554 B]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-11-8 11.8.86-1 [119 MB]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2022.3.0 2022.3.0.22-1 [580 MB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-11-8 11.8.0-1 [3,790 B]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2022.4.2 2022.4.2.50-32196742v0 [286 MB]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-11-8 11.8.0-1 [3,310 B]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-11-8 11.8.86-1 [81.4 kB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-11-8 11.8.87-1 [114 MB]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-11-8 11.8.0-1 [2,870 B]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-11-8 1.4.0.31-1 [38.7 MB]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-11-8 11.8.0-1 [2,390 B]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-8 11.8.0-1 [3,374 B]\n",
            "Fetched 2,715 MB in 44s (61.2 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-cccl-11-8.\n",
            "(Reading database ... 126333 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-cccl-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cupti-11-8.\n",
            "Preparing to unpack .../01-cuda-cupti-11-8_11.8.87-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-11-8 (11.8.87-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-11-8.\n",
            "Preparing to unpack .../02-cuda-cupti-dev-11-8_11.8.87-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-11-8 (11.8.87-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-11-8.\n",
            "Preparing to unpack .../03-cuda-nvdisasm-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-11-8.\n",
            "Preparing to unpack .../04-cuda-cuobjdump-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-gdb-11-8.\n",
            "Preparing to unpack .../05-cuda-gdb-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-memcheck-11-8.\n",
            "Preparing to unpack .../06-cuda-memcheck-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-memcheck-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-11-8.\n",
            "Preparing to unpack .../07-cuda-nvprof-11-8_11.8.87-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-11-8 (11.8.87-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-11-8.\n",
            "Preparing to unpack .../08-cuda-nvtx-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-11-8.\n",
            "Preparing to unpack .../09-cuda-sanitizer-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-11-8.\n",
            "Preparing to unpack .../10-cuda-command-line-tools-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-11-8.\n",
            "Preparing to unpack .../11-cuda-cuxxfilt-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-config-common.\n",
            "Preparing to unpack .../12-cuda-toolkit-11-config-common_11.8.89-1_all.deb ...\n",
            "Unpacking cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-8-config-common.\n",
            "Preparing to unpack .../13-cuda-toolkit-11-8-config-common_11.8.89-1_all.deb ...\n",
            "Unpacking cuda-toolkit-11-8-config-common (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-11-8.\n",
            "Preparing to unpack .../14-cuda-cudart-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-11-8.\n",
            "Preparing to unpack .../15-cuda-driver-dev-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-11-8.\n",
            "Preparing to unpack .../16-cuda-cudart-dev-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-11-8.\n",
            "Preparing to unpack .../17-cuda-nvcc-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-11-8.\n",
            "Preparing to unpack .../18-cuda-nvprune-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-compiler-11-8.\n",
            "Preparing to unpack .../19-cuda-compiler-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package cuda-documentation-11-8.\n",
            "Preparing to unpack .../20-cuda-documentation-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-11-8.\n",
            "Preparing to unpack .../21-cuda-nvrtc-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package libcublas-11-8.\n",
            "Preparing to unpack .../22-libcublas-11-8_11.11.3.6-1_amd64.deb ...\n",
            "Unpacking libcublas-11-8 (11.11.3.6-1) ...\n",
            "Selecting previously unselected package libcufft-11-8.\n",
            "Preparing to unpack .../23-libcufft-11-8_10.9.0.58-1_amd64.deb ...\n",
            "Unpacking libcufft-11-8 (10.9.0.58-1) ...\n",
            "Selecting previously unselected package libcufile-11-8.\n",
            "Preparing to unpack .../24-libcufile-11-8_1.4.0.31-1_amd64.deb ...\n",
            "Unpacking libcufile-11-8 (1.4.0.31-1) ...\n",
            "Selecting previously unselected package libcurand-11-8.\n",
            "Preparing to unpack .../25-libcurand-11-8_10.3.0.86-1_amd64.deb ...\n",
            "Unpacking libcurand-11-8 (10.3.0.86-1) ...\n",
            "Selecting previously unselected package libcusolver-11-8.\n",
            "Preparing to unpack .../26-libcusolver-11-8_11.4.1.48-1_amd64.deb ...\n",
            "Unpacking libcusolver-11-8 (11.4.1.48-1) ...\n",
            "Selecting previously unselected package libcusparse-11-8.\n",
            "Preparing to unpack .../27-libcusparse-11-8_11.7.5.86-1_amd64.deb ...\n",
            "Unpacking libcusparse-11-8 (11.7.5.86-1) ...\n",
            "Selecting previously unselected package libnpp-11-8.\n",
            "Preparing to unpack .../28-libnpp-11-8_11.8.0.86-1_amd64.deb ...\n",
            "Unpacking libnpp-11-8 (11.8.0.86-1) ...\n",
            "Selecting previously unselected package libnvjpeg-11-8.\n",
            "Preparing to unpack .../29-libnvjpeg-11-8_11.9.0.86-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-11-8 (11.9.0.86-1) ...\n",
            "Selecting previously unselected package cuda-libraries-11-8.\n",
            "Preparing to unpack .../30-cuda-libraries-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-11-8.\n",
            "Preparing to unpack .../31-cuda-profiler-api-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-11-8.\n",
            "Preparing to unpack .../32-cuda-nvrtc-dev-11-8_11.8.89-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-11-8 (11.8.89-1) ...\n",
            "Selecting previously unselected package libcublas-dev-11-8.\n",
            "Preparing to unpack .../33-libcublas-dev-11-8_11.11.3.6-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-11-8 (11.11.3.6-1) ...\n",
            "Selecting previously unselected package libcufft-dev-11-8.\n",
            "Preparing to unpack .../34-libcufft-dev-11-8_10.9.0.58-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-11-8 (10.9.0.58-1) ...\n",
            "Selecting previously unselected package libcufile-dev-11-8.\n",
            "Preparing to unpack .../35-libcufile-dev-11-8_1.4.0.31-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-11-8 (1.4.0.31-1) ...\n",
            "Selecting previously unselected package libcurand-dev-11-8.\n",
            "Preparing to unpack .../36-libcurand-dev-11-8_10.3.0.86-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-11-8 (10.3.0.86-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-11-8.\n",
            "Preparing to unpack .../37-libcusolver-dev-11-8_11.4.1.48-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-11-8 (11.4.1.48-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-11-8.\n",
            "Preparing to unpack .../38-libcusparse-dev-11-8_11.7.5.86-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-11-8 (11.7.5.86-1) ...\n",
            "Selecting previously unselected package libnpp-dev-11-8.\n",
            "Preparing to unpack .../39-libnpp-dev-11-8_11.8.0.86-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-11-8 (11.8.0.86-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-11-8.\n",
            "Preparing to unpack .../40-libnvjpeg-dev-11-8_11.9.0.86-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-11-8 (11.9.0.86-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-11-8.\n",
            "Preparing to unpack .../41-cuda-libraries-dev-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../42-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../43-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../44-openjdk-11-jre_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../45-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-11-8.\n",
            "Preparing to unpack .../46-cuda-nsight-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package nsight-compute-2022.3.0.\n",
            "Preparing to unpack .../47-nsight-compute-2022.3.0_2022.3.0.22-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2022.3.0 (2022.3.0.22-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-11-8.\n",
            "Preparing to unpack .../48-cuda-nsight-compute-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../49-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../50-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../51-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../52-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../53-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../54-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../55-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../56-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../57-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../58-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package nsight-systems-2022.4.2.\n",
            "Preparing to unpack .../59-nsight-systems-2022.4.2_2022.4.2.50-32196742v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2022.4.2 (2022.4.2.50-32196742v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-11-8.\n",
            "Preparing to unpack .../60-cuda-nsight-systems-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-11-8.\n",
            "Preparing to unpack .../61-cuda-nvml-dev-11-8_11.8.86-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-11-8 (11.8.86-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-11-8.\n",
            "Preparing to unpack .../62-cuda-nvvp-11-8_11.8.87-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-11-8 (11.8.87-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-11-8.\n",
            "Preparing to unpack .../63-cuda-visual-tools-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package gds-tools-11-8.\n",
            "Preparing to unpack .../64-gds-tools-11-8_1.4.0.31-1_amd64.deb ...\n",
            "Unpacking gds-tools-11-8 (1.4.0.31-1) ...\n",
            "Selecting previously unselected package cuda-tools-11-8.\n",
            "Preparing to unpack .../65-cuda-tools-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-tools-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-11-8.\n",
            "Preparing to unpack .../66-cuda-toolkit-11-8_11.8.0-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-11-8 (11.8.0-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../67-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../68-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../69-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../70-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../71-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../72-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-nvml-dev-11-8 (11.8.86-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up cuda-toolkit-11-config-common (11.8.89-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-cccl-11-8 (11.8.89-1) ...\n",
            "Setting up cuda-cuobjdump-11-8 (11.8.86-1) ...\n",
            "Setting up cuda-nvrtc-11-8 (11.8.89-1) ...\n",
            "Setting up cuda-sanitizer-11-8 (11.8.86-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-cupti-11-8 (11.8.87-1) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up cuda-nvdisasm-11-8 (11.8.86-1) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up cuda-cuxxfilt-11-8 (11.8.86-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-nvvp-11-8 (11.8.87-1) ...\n",
            "Setting up cuda-nvtx-11-8 (11.8.86-1) ...\n",
            "Setting up cuda-gdb-11-8 (11.8.86-1) ...\n",
            "Setting up cuda-toolkit-11-8-config-common (11.8.89-1) ...\n",
            "Setting alternatives\n",
            "update-alternatives: using /usr/local/cuda-11.8 to provide /usr/local/cuda-11 (cuda-11) in auto mode\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libcusolver-11-8 (11.4.1.48-1) ...\n",
            "Setting up cuda-nvrtc-dev-11-8 (11.8.89-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up cuda-driver-dev-11-8 (11.8.89-1) ...\n",
            "Setting up cuda-memcheck-11-8 (11.8.86-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up gds-tools-11-8 (1.4.0.31-1) ...\n",
            "Setting up cuda-nsight-11-8 (11.8.86-1) ...\n",
            "Setting up cuda-profiler-api-11-8 (11.8.86-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up cuda-documentation-11-8 (11.8.86-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-nvprune-11-8 (11.8.86-1) ...\n",
            "Setting up cuda-cudart-11-8 (11.8.89-1) ...\n",
            "Setting up libnvjpeg-11-8 (11.9.0.86-1) ...\n",
            "Setting up cuda-nvprof-11-8 (11.8.87-1) ...\n",
            "Setting up nsight-compute-2022.3.0 (2022.3.0.22-1) ...\n",
            "Setting up nsight-systems-2022.4.2 (2022.4.2.50-32196742v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2022.4.2/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2022.4.2/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libcusparse-11-8 (11.7.5.86-1) ...\n",
            "Setting up libcufft-11-8 (10.9.0.58-1) ...\n",
            "Setting up cuda-cupti-dev-11-8 (11.8.87-1) ...\n",
            "Setting up libcufft-dev-11-8 (10.9.0.58-1) ...\n",
            "Setting up cuda-cudart-dev-11-8 (11.8.89-1) ...\n",
            "Setting up libnpp-11-8 (11.8.0.86-1) ...\n",
            "Setting up libcusolver-dev-11-8 (11.4.1.48-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-nsight-systems-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-command-line-tools-11-8 (11.8.0-1) ...\n",
            "Setting up libcusparse-dev-11-8 (11.7.5.86-1) ...\n",
            "Setting up libcurand-11-8 (10.3.0.86-1) ...\n",
            "Setting up libcufile-11-8 (1.4.0.31-1) ...\n",
            "Setting alternatives\n",
            "Setting up libcublas-11-8 (11.11.3.6-1) ...\n",
            "Setting up libnpp-dev-11-8 (11.8.0.86-1) ...\n",
            "Setting up cuda-libraries-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-nsight-compute-11-8 (11.8.0-1) ...\n",
            "Setting up libnvjpeg-dev-11-8 (11.9.0.86-1) ...\n",
            "Setting up cuda-nvcc-11-8 (11.8.89-1) ...\n",
            "Setting up libcublas-dev-11-8 (11.11.3.6-1) ...\n",
            "Setting up libcurand-dev-11-8 (10.3.0.86-1) ...\n",
            "Setting up libcufile-dev-11-8 (1.4.0.31-1) ...\n",
            "Setting up cuda-compiler-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-libraries-dev-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-visual-tools-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-tools-11-8 (11.8.0-1) ...\n",
            "Setting up cuda-toolkit-11-8 (11.8.0-1) ...\n",
            "Setting alternatives\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install -y build-essential\n",
        "!apt install -y cuda-toolkit-11-8\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azpA1jPGxTbR",
        "outputId": "82d5f910-80a2-4d4e-c247-7d6602c8033c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.7)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.13.2)\n",
            "Downloading pytools-2025.1.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660426 sha256=37a3456a69d32a37ea638c0ba0275ce70324a8af5af0d9341c30877256621791\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, pycuda\n",
            "Successfully installed pycuda-2025.1 pytools-2025.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit # Initializes CUDA context\n",
        "import pycuda.compiler as compiler\n",
        "import numpy as np\n",
        "import time\n",
        "import collections\n",
        "import heapq\n",
        "\n",
        "# Check PyCUDA initialization\n",
        "try:\n",
        "    print(f\"PyCUDA initialized successfully on device: {drv.Device(0).name()}\")\n",
        "    !nvidia-smi # Display GPU information again\n",
        "except Exception as e:\n",
        "    print(\"Error initializing PyCUDA:\", e)\n",
        "    print(\"Please ensure CUDA drivers and toolkit are compatible and a GPU runtime is selected.\")\n",
        "    # import sys\n",
        "    # sys.exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Ua0QYV5Cnh",
        "outputId": "c4f37fa5-d5a5-4bf5-e699-3723a7fe7513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyCUDA initialized successfully on device: Tesla T4\n",
            "Sat Apr 26 21:00:07 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0             26W /   70W |     206MiB /  15360MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Graph Representation (CSR Format) and Sample Graph\n",
        "\n",
        "\n",
        "\n",
        "def create_sample_graph_csr(num_nodes, num_edges, directed=False, seed=42):\n",
        "    \"\"\"Creates a random sparse graph and returns its CSR representation.\"\"\"\n",
        "    print(f\"Creating a sample graph: {num_nodes} nodes, ~{num_edges} edges...\")\n",
        "    np.random.seed(seed)\n",
        "    max_edges = num_nodes * (num_nodes - 1)\n",
        "    if not directed: max_edges //= 2\n",
        "    num_edges = min(num_edges, max_edges)\n",
        "    sources = np.random.randint(0, num_nodes, num_edges * 2)\n",
        "    targets = np.random.randint(0, num_nodes, num_edges * 2)\n",
        "    mask = sources != targets\n",
        "    sources = sources[mask][:num_edges]\n",
        "    targets = targets[mask][:num_edges]\n",
        "    if not directed:\n",
        "        sources_full = np.concatenate([sources, targets])\n",
        "        targets_full = np.concatenate([targets, sources])\n",
        "    else:\n",
        "        sources_full = sources\n",
        "        targets_full = targets\n",
        "    weights = np.random.randint(1, 11, size=len(sources_full)).astype(np.float32)\n",
        "    sorted_indices = np.argsort(sources_full)\n",
        "    sources_sorted = sources_full[sorted_indices]\n",
        "    targets_sorted = targets_full[sorted_indices]\n",
        "    weights_sorted = weights[sorted_indices]\n",
        "    indptr = np.zeros(num_nodes + 1, dtype=np.int32)\n",
        "    indices = np.zeros(len(targets_sorted), dtype=np.int32)\n",
        "    data = np.zeros(len(weights_sorted), dtype=np.float32)\n",
        "    current_edge_idx = 0\n",
        "    last_source = -1\n",
        "    for i in range(len(sources_sorted)):\n",
        "        source = sources_sorted[i]\n",
        "        target = targets_sorted[i]\n",
        "        weight = weights_sorted[i]\n",
        "        while last_source < source:\n",
        "            last_source += 1\n",
        "            indptr[last_source] = current_edge_idx\n",
        "        indices[current_edge_idx] = target\n",
        "        data[current_edge_idx] = weight\n",
        "        current_edge_idx += 1\n",
        "    while last_source < num_nodes:\n",
        "        last_source += 1\n",
        "        indptr[last_source] = current_edge_idx\n",
        "    indptr[num_nodes] = current_edge_idx\n",
        "    indices = indices[:current_edge_idx]\n",
        "    data = data[:current_edge_idx]\n",
        "    print(f\"Actual number of edges stored (considering directionality): {current_edge_idx}\")\n",
        "    print(\"Graph representation (CSR):\")\n",
        "    print(f\"  indptr (shape {indptr.shape}, {indptr.dtype})\")\n",
        "    print(f\"  indices (shape {indices.shape}, {indices.dtype})\")\n",
        "    print(f\"  data (shape {data.shape}, {data.dtype})\")\n",
        "    return indptr, indices, data, num_nodes\n",
        "\n",
        "# --- Create a SMALL graph for demonstration ---\n",
        "NUM_NODES = 1024 * 4 # Increase size slightly for better comparison\n",
        "AVG_DEGREE = 16\n",
        "NUM_EDGES = NUM_NODES * AVG_DEGREE\n",
        "\n",
        "csr_indptr_cpu, csr_indices_cpu, csr_weights_cpu, N = create_sample_graph_csr(NUM_NODES, NUM_EDGES, directed=False)\n",
        "\n",
        "# --- Helper function for PyCUDA kernel launch config ---\n",
        "def get_kernel_config_pycuda(n_items, threads_per_block=256):\n",
        "    \"\"\" Calculates grid and block dimensions for PyCUDA. \"\"\"\n",
        "    blocks_per_grid = (n_items + threads_per_block - 1) // threads_per_block\n",
        "    return (int(blocks_per_grid), 1, 1), (int(threads_per_block), 1, 1) # PyCUDA expects tuples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6LF3lXi5o7b",
        "outputId": "0553b48c-c893-4c2a-9786-306ff5efa3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a sample graph: 4096 nodes, ~65536 edges...\n",
            "Actual number of edges stored (considering directionality): 131072\n",
            "Graph representation (CSR):\n",
            "  indptr (shape (4097,), int32)\n",
            "  indices (shape (131072,), int32)\n",
            "  data (shape (131072,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. PyCUDA Kernels and Host Code\n",
        "\n",
        "# --- 2.1 Breadth-First Search (BFS) with PyCUDA ---\n",
        "\n",
        "bfs_kernel_code_pycuda = r'''\n",
        "// Same kernel code as before\n",
        "extern \"C\" __global__\n",
        "void bfs_kernel(const int* indptr, const int* indices, int* dist, int* frontier_in, int* frontier_out, int* frontier_size_out, int current_level, int num_nodes) {\n",
        "\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Read current frontier size ONCE from global memory (atomically updated by host/other blocks)\n",
        "    // Use volatile to prevent compiler optimizing reads, though atomic access handles memory visibility.\n",
        "    // int current_frontier_size = frontier_size_out[0]; // This was incorrect logic in CuPy version too for kernel boundary check!\n",
        "    // The check needs to be against the size *before* the kernel started adding to frontier_out.\n",
        "    // Let's assume the grid size was calculated correctly based on frontier_size_in\n",
        "    // The size passed in frontier_size_out[0] is the *input* size for this kernel launch.\n",
        "\n",
        "    if (tid < frontier_size_out[0]) { // Check if thread ID is within the input frontier bounds\n",
        "         int u = frontier_in[tid]; // Get node 'u' from the current frontier\n",
        "\n",
        "         if (u < 0 || u >= num_nodes) return; // Boundary check for safety\n",
        "\n",
        "         int start_edge = indptr[u];\n",
        "         int end_edge = indptr[u + 1];\n",
        "\n",
        "         for (int i = start_edge; i < end_edge; ++i) {\n",
        "             int v = indices[i]; // Neighbor node 'v'\n",
        "\n",
        "             if (v < 0 || v >= num_nodes) continue; // Boundary check\n",
        "\n",
        "             int old_dist = atomicCAS(&dist[v], -1, current_level + 1);\n",
        "\n",
        "             if (old_dist == -1) {\n",
        "                 // Add 'v' to the *next* frontier using atomicAdd to get a unique index\n",
        "                 int index = atomicAdd(&frontier_size_out[1], 1); // Index for next frontier size\n",
        "                 // *** Potential Issue: If next frontier exceeds allocation size, this writes out of bounds! ***\n",
        "                 // A robust implementation needs size checks or dynamic resizing (complex).\n",
        "                 // Assuming frontier arrays are allocated large enough (num_nodes).\n",
        "                 frontier_out[index] = v; // Add 'v' to the next frontier array\n",
        "             }\n",
        "         }\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile the kernel\n",
        "bfs_module = compiler.SourceModule(bfs_kernel_code_pycuda)\n",
        "bfs_kernel_pycuda = bfs_module.get_function(\"bfs_kernel\")\n",
        "\n",
        "def run_bfs_gpu_timed_pycuda(indptr_cpu, indices_cpu, num_nodes, start_node=0):\n",
        "    \"\"\" Executes BFS on the GPU using PyCUDA with timing \"\"\"\n",
        "    print(f\"\\nRunning BFS from node {start_node} on GPU (PyCUDA)...\")\n",
        "\n",
        "    # Allocate GPU memory\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes)\n",
        "    d_dist = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier1 = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier2 = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier_size = drv.mem_alloc(2 * np.int32().itemsize) # [current_size, next_size]\n",
        "\n",
        "    # Transfer initial data H->D\n",
        "    drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    drv.memcpy_htod(d_indices, indices_cpu)\n",
        "\n",
        "    # Initialize dist array (-1)\n",
        "    dist_init_cpu = np.full(num_nodes, -1, dtype=np.int32)\n",
        "    drv.memcpy_htod(d_dist, dist_init_cpu)\n",
        "\n",
        "    # Initialize frontier size array ([0, 0])\n",
        "    frontier_size_init_cpu = np.zeros(2, dtype=np.int32)\n",
        "    drv.memcpy_htod(d_frontier_size, frontier_size_init_cpu)\n",
        "\n",
        "    # Setup initial state on CPU first, then transfer\n",
        "    dist_init_cpu[start_node] = 0\n",
        "    drv.memcpy_htod(d_dist, dist_init_cpu) # Transfer updated dist\n",
        "\n",
        "    frontier1_init_cpu = np.zeros(num_nodes, dtype=np.int32) # Only need first element\n",
        "    frontier1_init_cpu[0] = start_node\n",
        "    drv.memcpy_htod(d_frontier1, frontier1_init_cpu) # Transfer updated frontier1\n",
        "\n",
        "    frontier_size_init_cpu[0] = 1 # Set current size to 1\n",
        "    drv.memcpy_htod(d_frontier_size, frontier_size_init_cpu) # Transfer updated size\n",
        "\n",
        "    current_level = 0\n",
        "    threads_per_block = 256\n",
        "\n",
        "    frontier_in_gpu = d_frontier1\n",
        "    frontier_out_gpu = d_frontier2\n",
        "\n",
        "    # Timing events\n",
        "    start_event = drv.Event()\n",
        "    end_event = drv.Event()\n",
        "    total_gpu_time = 0.0\n",
        "\n",
        "    # Temporary CPU array for frontier size\n",
        "    frontier_size_h = np.zeros(2, dtype=np.int32)\n",
        "    frontier_size_h_reset_next = np.zeros(2, dtype=np.int32) # Used to reset next_size to 0\n",
        "\n",
        "    # --- Main BFS loop (level-synchronous) ---\n",
        "    start_event.record() # Record event before the loop\n",
        "    while True:\n",
        "        # Read current frontier size D->H\n",
        "        drv.memcpy_dtoh(frontier_size_h, d_frontier_size)\n",
        "        current_frontier_size = int(frontier_size_h[0])\n",
        "\n",
        "        if current_frontier_size == 0:\n",
        "            break # No more nodes to visit\n",
        "\n",
        "        # Reset next frontier size counter ([current, 0]) H->D\n",
        "        frontier_size_h_reset_next[0] = current_frontier_size # Keep current size\n",
        "        frontier_size_h_reset_next[1] = 0                     # Reset next size\n",
        "        drv.memcpy_htod(d_frontier_size, frontier_size_h_reset_next)\n",
        "\n",
        "        # Configure and launch kernel\n",
        "        grid_dim, block_dim = get_kernel_config_pycuda(current_frontier_size, threads_per_block)\n",
        "\n",
        "        # Prepare args: Pointers and explicitly typed scalars\n",
        "        args = [d_indptr, d_indices, d_dist, frontier_in_gpu, frontier_out_gpu,\n",
        "                d_frontier_size, np.int32(current_level), np.int32(num_nodes)]\n",
        "        bfs_kernel_pycuda(*args, block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Read the size of the *next* frontier D->H (needed to set current size for next iter)\n",
        "        drv.memcpy_dtoh(frontier_size_h, d_frontier_size) # This syncs kernel implicitly\n",
        "        next_frontier_size = int(frontier_size_h[1])\n",
        "\n",
        "        # Update current frontier size on GPU H->D ([next, next])\n",
        "        frontier_size_h[0] = next_frontier_size # Set current = next for next loop\n",
        "        # frontier_size_h[1] is already next_frontier_size from the memcpy_dtoh above\n",
        "        drv.memcpy_htod(d_frontier_size, frontier_size_h)\n",
        "\n",
        "        # Swap frontier pointers (no data movement)\n",
        "        frontier_in_gpu, frontier_out_gpu = frontier_out_gpu, frontier_in_gpu\n",
        "\n",
        "        current_level += 1\n",
        "\n",
        "    end_event.record() # Record event after the loop\n",
        "    # --- End BFS Loop ---\n",
        "\n",
        "    # Wait for all GPU activity to complete\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0 # Time in seconds\n",
        "\n",
        "    # Retrieve results D->H\n",
        "    dist_result_cpu = np.empty_like(dist_init_cpu)\n",
        "    drv.memcpy_dtoh(dist_result_cpu, d_dist)\n",
        "\n",
        "    # Free GPU memory (important!)\n",
        "    d_indptr.free()\n",
        "    d_indices.free()\n",
        "    d_dist.free()\n",
        "    d_frontier1.free()\n",
        "    d_frontier2.free()\n",
        "    d_frontier_size.free()\n",
        "\n",
        "    print(f\"GPU BFS (PyCUDA) finished in {gpu_time:.4f} seconds.\")\n",
        "    unvisited_count = np.sum(dist_result_cpu == -1)\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {num_nodes - unvisited_count}\")\n",
        "\n",
        "    return dist_result_cpu, gpu_time\n",
        "\n",
        "\n",
        "# --- 2.2 Dijkstra's Algorithm with PyCUDA ---\n",
        "\n",
        "dijkstra_kernel_code_pycuda = r'''\n",
        "#include <float.h> // For FLT_MAX\n",
        "\n",
        "// Need bool definition for kernel if not included elsewhere\n",
        "// typedef unsigned char bool;\n",
        "// #define true 1\n",
        "// #define false 0\n",
        "// PyCUDA seems to handle bool okay usually.\n",
        "\n",
        "// atomicCAS for float simulation\n",
        "static __device__ float atomicMinFloat(float* addr, float value) {\n",
        "    float old;\n",
        "    unsigned int* addr_as_int = (unsigned int*)addr;\n",
        "    unsigned int old_int = *addr_as_int;\n",
        "    unsigned int assumed;\n",
        "    unsigned int value_as_int = __float_as_int(value);\n",
        "\n",
        "    do {\n",
        "        assumed = old_int;\n",
        "        old = __int_as_float(old_int);\n",
        "        if (value >= old) { // Our value is not better\n",
        "            break;\n",
        "        }\n",
        "        // Try to swap using atomicCAS, comparing integer representations\n",
        "        old_int = atomicCAS(addr_as_int, assumed, value_as_int);\n",
        "    } while (assumed != old_int); // Loop if CAS failed\n",
        "\n",
        "    return old; // Return the value previously stored\n",
        "}\n",
        "\n",
        "\n",
        "extern \"C\" __global__\n",
        "void dijkstra_relax_kernel(const int* indptr, const int* indices, const float* weights,\n",
        "                           float* dist, const bool* active_nodes_in, bool* active_nodes_out,\n",
        "                           bool* changed, int num_nodes) {\n",
        "\n",
        "    int u = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (u >= num_nodes) return; // Boundary check first\n",
        "\n",
        "    bool is_active = active_nodes_in[u];\n",
        "    active_nodes_out[u] = is_active; // Initialize output state\n",
        "\n",
        "    if (!is_active) { // Process only active nodes\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    // Node 'u' is active in this iteration. Assume it won't activate others.\n",
        "    active_nodes_out[u] = false; // Reset output state for 'u'\n",
        "    float u_dist = dist[u];\n",
        "\n",
        "    if (u_dist == FLT_MAX) return;\n",
        "\n",
        "    int start_edge = indptr[u];\n",
        "    int end_edge = indptr[u + 1];\n",
        "\n",
        "    for (int i = start_edge; i < end_edge; ++i) {\n",
        "        int v = indices[i];\n",
        "        float weight = weights[i];\n",
        "\n",
        "        if (v < 0 || v >= num_nodes) continue;\n",
        "\n",
        "        float new_dist_v = u_dist + weight;\n",
        "\n",
        "        // Use the atomicMinFloat helper function\n",
        "        float old_dist_v = atomicMinFloat(&dist[v], new_dist_v);\n",
        "\n",
        "        // If we successfully updated the distance (new_dist_v was smaller than old_dist_v)\n",
        "        if (new_dist_v < old_dist_v) {\n",
        "             active_nodes_out[v] = true; // Mark neighbor 'v' as active for the next iteration\n",
        "             // Indicate that *some* change occurred using atomicCAS on the bool flag (as int)\n",
        "             atomicCAS((unsigned int*)&changed[0], 0, 1); // Set changed[0] = true (1)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile the kernel\n",
        "dijkstra_module = compiler.SourceModule(dijkstra_kernel_code_pycuda)\n",
        "dijkstra_relax_kernel_pycuda = dijkstra_module.get_function(\"dijkstra_relax_kernel\")\n",
        "\n",
        "\n",
        "def run_dijkstra_gpu_timed_pycuda(indptr_cpu, indices_cpu, weights_cpu, num_nodes, start_node=0, max_iterations=1000):\n",
        "    \"\"\" Executes Dijkstra on the GPU using PyCUDA with timing \"\"\"\n",
        "    print(f\"\\nRunning Dijkstra from node {start_node} on GPU (PyCUDA)...\")\n",
        "\n",
        "    infinity = np.finfo(np.float32).max\n",
        "\n",
        "    # Allocate GPU memory\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes)\n",
        "    d_weights = drv.mem_alloc(weights_cpu.nbytes)\n",
        "    d_dist = drv.mem_alloc(num_nodes * np.float32().itemsize)\n",
        "    # PyCUDA bool handling: Often maps to uint8\n",
        "    bool_itemsize = np.dtype(np.bool_).itemsize\n",
        "    d_active1 = drv.mem_alloc(num_nodes * bool_itemsize)\n",
        "    d_active2 = drv.mem_alloc(num_nodes * bool_itemsize)\n",
        "    d_changed = drv.mem_alloc(1 * bool_itemsize) # Single boolean flag\n",
        "\n",
        "    # Transfer static data H->D\n",
        "    drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    drv.memcpy_htod(d_indices, indices_cpu)\n",
        "    drv.memcpy_htod(d_weights, weights_cpu)\n",
        "\n",
        "    # Initialize CPU arrays\n",
        "    dist_cpu = np.full(num_nodes, infinity, dtype=np.float32)\n",
        "    active1_cpu = np.zeros(num_nodes, dtype=np.bool_)\n",
        "    active2_cpu = np.zeros(num_nodes, dtype=np.bool_) # Not strictly needed for init\n",
        "    changed_cpu = np.zeros(1, dtype=np.bool_)\n",
        "\n",
        "    # Set initial state\n",
        "    dist_cpu[start_node] = 0.0\n",
        "    active1_cpu[start_node] = True\n",
        "\n",
        "    # Transfer initial state H->D\n",
        "    drv.memcpy_htod(d_dist, dist_cpu)\n",
        "    drv.memcpy_htod(d_active1, active1_cpu)\n",
        "    # No need to transfer active2 initially\n",
        "\n",
        "    active_in_gpu = d_active1\n",
        "    active_out_gpu = d_active2\n",
        "\n",
        "    threads_per_block = 256\n",
        "    grid_dim, block_dim = get_kernel_config_pycuda(num_nodes, threads_per_block)\n",
        "\n",
        "    # Timing events\n",
        "    start_event = drv.Event()\n",
        "    end_event = drv.Event()\n",
        "    iterations_done = 0\n",
        "\n",
        "    # --- Main Relaxation Loop ---\n",
        "    start_event.record()\n",
        "    for iteration in range(max_iterations):\n",
        "        iterations_done = iteration + 1\n",
        "\n",
        "        # Reset changed flag H->D (set to false/0)\n",
        "        changed_cpu[0] = False\n",
        "        drv.memcpy_htod(d_changed, changed_cpu)\n",
        "\n",
        "        # Launch relaxation kernel\n",
        "        # Note: Pass bool arrays directly. PyCUDA handles pointer passing.\n",
        "        # Scalars need explicit numpy typing.\n",
        "        args = [d_indptr, d_indices, d_weights, d_dist,\n",
        "                active_in_gpu, active_out_gpu, d_changed, np.int32(num_nodes)]\n",
        "        dijkstra_relax_kernel_pycuda(*args, block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Check if any distances were updated D->H\n",
        "        drv.memcpy_dtoh(changed_cpu, d_changed) # This syncs kernel implicitly\n",
        "        if not changed_cpu[0]:\n",
        "            print(f\"Convergence reached after {iteration + 1} iterations.\")\n",
        "            break\n",
        "\n",
        "        # Swap active node pointers\n",
        "        active_in_gpu, active_out_gpu = active_out_gpu, active_in_gpu\n",
        "\n",
        "    else: # Loop finished without break\n",
        "        print(f\"Warning: Max iterations ({max_iterations}) reached without full convergence.\")\n",
        "\n",
        "    end_event.record()\n",
        "    # --- End Relaxation Loop ---\n",
        "\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0\n",
        "\n",
        "    # Retrieve results D->H\n",
        "    dist_result_cpu = np.empty_like(dist_cpu)\n",
        "    drv.memcpy_dtoh(dist_result_cpu, d_dist)\n",
        "\n",
        "    # Free GPU memory\n",
        "    d_indptr.free()\n",
        "    d_indices.free()\n",
        "    d_weights.free()\n",
        "    d_dist.free()\n",
        "    d_active1.free()\n",
        "    d_active2.free()\n",
        "    d_changed.free()\n",
        "\n",
        "    print(f\"GPU Dijkstra (PyCUDA) finished in {gpu_time:.4f} seconds after {iterations_done} iterations.\")\n",
        "    reachable_count = np.sum(dist_result_cpu != infinity)\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {reachable_count}\")\n",
        "    if iterations_done == max_iterations and changed_cpu[0]:\n",
        "         print(f\"Warning: Max iterations ({max_iterations}) reached without full convergence.\")\n",
        "\n",
        "\n",
        "    return dist_result_cpu, gpu_time\n",
        "\n",
        "\n",
        "# --- 2.3 PageRank with PyCUDA ---\n",
        "\n",
        "# Requires a reduction kernel for convergence check\n",
        "pagerank_kernels_pycuda = r'''\n",
        "// Includes kernel from CuPy version + reduction kernel\n",
        "\n",
        "// --- Kernel 1: Calculate Contributions & Dangling Sum ---\n",
        "extern \"C\" __global__\n",
        "void pagerank_kernel(const int* indptr, const int* indices, const float* ranks_in, float* ranks_out,\n",
        "                      const int* out_degree, float* dangling_sum, // dangling_sum[0] used for atomic update\n",
        "                      int num_nodes) {\n",
        "\n",
        "    int u = blockIdx.x * blockDim.x + threadIdx.x; // Thread corresponds to node 'u'\n",
        "    if (u >= num_nodes) return;\n",
        "\n",
        "    float rank_u = ranks_in[u];\n",
        "    int degree_u = out_degree[u];\n",
        "\n",
        "    if (degree_u > 0) {\n",
        "        float contribution = rank_u / degree_u;\n",
        "        int start_edge = indptr[u];\n",
        "        int end_edge = indptr[u + 1];\n",
        "        for (int i = start_edge; i < end_edge; ++i) {\n",
        "            int v = indices[i];\n",
        "            if (v >= 0 && v < num_nodes) {\n",
        "                 atomicAdd(&ranks_out[v], contribution); // Add contribution to target node's slot\n",
        "            }\n",
        "        }\n",
        "    } else {\n",
        "        // Dangling node: add its rank to the global dangling sum\n",
        "        atomicAdd(&dangling_sum[0], rank_u);\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- Kernel 2: Apply Damping Factor & Dangling Contribution ---\n",
        "extern \"C\" __global__\n",
        "void pagerank_finish_kernel(float* ranks_out, const float* dangling_sum, // Read dangling_sum[0]\n",
        "                             int num_nodes, float damping_factor, float base_rank) {\n",
        "\n",
        "    int v = blockIdx.x * blockDim.x + threadIdx.x; // Thread corresponds to node 'v'\n",
        "    if (v >= num_nodes) return;\n",
        "\n",
        "    float total_dangling_rank = dangling_sum[0]; // Read accumulated sum\n",
        "    float dangling_contribution = (num_nodes > 0) ? (total_dangling_rank / num_nodes) : 0.0f;\n",
        "\n",
        "    // ranks_out[v] currently holds the raw sum of contributions from neighbors (from pagerank_kernel)\n",
        "    ranks_out[v] = base_rank + damping_factor * (ranks_out[v] + dangling_contribution);\n",
        "}\n",
        "\n",
        "\n",
        "// --- Kernel 3: Reduction for Convergence Check ---\n",
        "// Computes Sum(Abs(arr1 - arr2))\n",
        "__device__ volatile float reduction_shared_mem[256]; // Max threads per block assumed\n",
        "\n",
        "extern \"C\" __global__\n",
        "void reduce_diff_kernel(const float* arr1, const float* arr2, float* result, int n) {\n",
        "    // extern __shared__ float sdata[]; // Using static shared mem for fixed size\n",
        "    volatile float* sdata = reduction_shared_mem; // Point to static shared memory and add volatile\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; // Global index\n",
        "    unsigned int gridSize = gridDim.x * blockDim.x; // Total number of threads in grid\n",
        "\n",
        "    float my_sum = 0.0f;\n",
        "\n",
        "    // Each thread computes sum for multiple elements if n > gridSize\n",
        "    while (i < n) {\n",
        "        my_sum += fabsf(arr1[i] - arr2[i]);\n",
        "        i += gridSize;\n",
        "    }\n",
        "    sdata[tid] = my_sum; // Store thread's partial sum\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction in shared memory\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Block leader adds block's sum to global result atomically\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(result, sdata[0]); // Add block's sum to result[0]\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile kernels\n",
        "pagerank_module = compiler.SourceModule(pagerank_kernels_pycuda)\n",
        "pagerank_kernel_pycuda = pagerank_module.get_function(\"pagerank_kernel\")\n",
        "pagerank_finish_kernel_pycuda = pagerank_module.get_function(\"pagerank_finish_kernel\")\n",
        "reduce_diff_kernel_pycuda = pagerank_module.get_function(\"reduce_diff_kernel\")\n",
        "\n",
        "\n",
        "def run_pagerank_gpu_timed_pycuda(indptr_cpu, indices_cpu, num_nodes, damping_factor=0.85, max_iterations=100, tolerance=1e-5):\n",
        "    \"\"\" Executes PageRank on the GPU using PyCUDA with timing \"\"\"\n",
        "    print(f\"\\nRunning PageRank (d={damping_factor}, tol={tolerance}, max_iter={max_iterations}) on GPU (PyCUDA)...\")\n",
        "\n",
        "    # Precompute Out-Degrees (CPU)\n",
        "    out_degree_cpu = np.zeros(num_nodes, dtype=np.int32)\n",
        "    for i in range(num_nodes):\n",
        "        out_degree_cpu[i] = indptr_cpu[i+1] - indptr_cpu[i]\n",
        "\n",
        "    # Allocate GPU memory\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes)\n",
        "    d_out_degree = drv.mem_alloc(out_degree_cpu.nbytes)\n",
        "    d_ranks1 = drv.mem_alloc(num_nodes * np.float32().itemsize) # Ranks In\n",
        "    d_ranks2 = drv.mem_alloc(num_nodes * np.float32().itemsize) # Ranks Out (buffer)\n",
        "    d_dangling_sum = drv.mem_alloc(1 * np.float32().itemsize) # Single float for atomic sum\n",
        "    d_diff_sum = drv.mem_alloc(1 * np.float32().itemsize)     # Single float for reduction result\n",
        "\n",
        "    # Transfer static data H->D\n",
        "    drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    drv.memcpy_htod(d_indices, indices_cpu)\n",
        "    drv.memcpy_htod(d_out_degree, out_degree_cpu)\n",
        "\n",
        "    # Initialize ranks H->D\n",
        "    ranks_init_cpu = np.full(num_nodes, 1.0 / num_nodes, dtype=np.float32)\n",
        "    drv.memcpy_htod(d_ranks1, ranks_init_cpu)\n",
        "\n",
        "    # Zero buffers for accumulation (use memset for efficiency)\n",
        "    drv.memset_d32(d_ranks2, 0, num_nodes) # Zero out ranks buffer\n",
        "    drv.memset_d32(d_dangling_sum, 0, 1)   # Zero out dangling sum\n",
        "    drv.memset_d32(d_diff_sum, 0, 1)       # Zero out diff sum buffer\n",
        "\n",
        "    ranks_in_gpu = d_ranks1\n",
        "    ranks_out_gpu = d_ranks2\n",
        "\n",
        "    base_rank = np.float32((1.0 - damping_factor) / num_nodes)\n",
        "    threads_per_block = 256\n",
        "    grid_dim, block_dim = get_kernel_config_pycuda(num_nodes, threads_per_block)\n",
        "    # Shared memory for reduction kernel (must be >= threads_per_block * sizeof(float))\n",
        "    shared_mem_bytes = threads_per_block * np.float32().itemsize\n",
        "\n",
        "    # Timing events\n",
        "    start_event = drv.Event()\n",
        "    end_event = drv.Event()\n",
        "    iterations_done = 0\n",
        "    diff_val = infinity = np.finfo(np.float32).max # Initialize diff\n",
        "\n",
        "    # --- Main PageRank Iteration Loop ---\n",
        "    start_event.record()\n",
        "    for iteration in range(max_iterations):\n",
        "        iterations_done = iteration + 1\n",
        "\n",
        "        # 1. Reset ranks_out buffer and dangling sum (already zeroed initially, need reset inside loop)\n",
        "        drv.memset_d32(ranks_out_gpu, 0, num_nodes)\n",
        "        drv.memset_d32(d_dangling_sum, 0, 1)\n",
        "\n",
        "        # 2. Launch kernel to calculate contributions and dangling sum\n",
        "        # Args: indptr, indices, ranks_in, ranks_out, out_degree, dangling_sum_arr, N\n",
        "        args_k1 = [d_indptr, d_indices, ranks_in_gpu, ranks_out_gpu, d_out_degree,\n",
        "                   d_dangling_sum, np.int32(num_nodes)]\n",
        "        pagerank_kernel_pycuda(*args_k1, block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # 3. Launch kernel to apply damping factor, base rank, and dangling contribution\n",
        "        # Args: ranks_out, dangling_sum_arr, N, d, base_rank\n",
        "        args_k2 = [ranks_out_gpu, d_dangling_sum, np.int32(num_nodes),\n",
        "                   np.float32(damping_factor), base_rank] # Pass base_rank correctly\n",
        "        pagerank_finish_kernel_pycuda(*args_k2, block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # 4. Check for convergence using reduction kernel\n",
        "        drv.memset_d32(d_diff_sum, 0, 1) # Reset diff sum buffer before reduction\n",
        "        # Args: ranks_out, ranks_in, result_buffer, N\n",
        "        args_k3 = [ranks_out_gpu, ranks_in_gpu, d_diff_sum, np.int32(num_nodes)]\n",
        "        reduce_diff_kernel_pycuda(*args_k3, block=block_dim, grid=grid_dim, shared=shared_mem_bytes)\n",
        "\n",
        "        # Retrieve the difference sum D->H\n",
        "        diff_sum_h = np.zeros(1, dtype=np.float32)\n",
        "        drv.memcpy_dtoh(diff_sum_h, d_diff_sum) # This syncs reduction kernel implicitly\n",
        "        diff_val = diff_sum_h[0]\n",
        "\n",
        "        if iteration % 10 == 0 or iteration == max_iterations - 1 or diff_val < tolerance:\n",
        "             print(f\"  Iteration {iteration+1}: Change = {diff_val:.6f}\")\n",
        "\n",
        "        if diff_val < tolerance:\n",
        "            print(f\"Convergence reached after {iteration + 1} iterations.\")\n",
        "            # ranks_out_gpu contains the final converged ranks, update pointer\n",
        "            ranks_in_gpu = ranks_out_gpu\n",
        "            break\n",
        "\n",
        "        # 5. Swap rank pointers (no data movement)\n",
        "        ranks_in_gpu, ranks_out_gpu = ranks_out_gpu, ranks_in_gpu\n",
        "    else: # Loop finished without break\n",
        "        # ranks_in_gpu holds the result from the last completed iteration\n",
        "        print(f\"Warning: Max iterations ({max_iterations}) reached without convergence.\")\n",
        "\n",
        "\n",
        "    end_event.record()\n",
        "    # --- End PageRank Loop ---\n",
        "\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0\n",
        "\n",
        "    # Retrieve final ranks D->H (pointed to by ranks_in_gpu)\n",
        "    ranks_result_cpu = np.empty_like(ranks_init_cpu)\n",
        "    drv.memcpy_dtoh(ranks_result_cpu, ranks_in_gpu)\n",
        "\n",
        "    # Free GPU memory\n",
        "    d_indptr.free()\n",
        "    d_indices.free()\n",
        "    d_out_degree.free()\n",
        "    d_ranks1.free()\n",
        "    d_ranks2.free()\n",
        "    d_dangling_sum.free()\n",
        "    d_diff_sum.free()\n",
        "\n",
        "    print(f\"GPU PageRank (PyCUDA) finished in {gpu_time:.4f} seconds after {iterations_done} iterations.\")\n",
        "    print(f\"Sum of ranks: {np.sum(ranks_result_cpu):.4f}\")\n",
        "    if iterations_done == max_iterations and diff_val >= tolerance:\n",
        "        print(f\"Warning: Max iterations ({max_iterations}) reached without convergence (last change: {diff_val:.6f}).\")\n",
        "\n",
        "    return ranks_result_cpu, gpu_time\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R1KKBpv5wgk",
        "outputId": "3d48fa73-1c42-4516-a8b4-a499d1bcb072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. PyCUDA Kernels and Host Code (Corrected PageRank Kernels)\n",
        "\n",
        "\n",
        "# --- BFS Code (No changes needed) ---\n",
        "bfs_kernel_code_pycuda = r'''\n",
        "extern \"C\" __global__\n",
        "void bfs_kernel(const int* indptr, const int* indices, int* dist, int* frontier_in, int* frontier_out, int* frontier_size_out, int current_level, int num_nodes) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < frontier_size_out[0]) {\n",
        "         int u = frontier_in[tid];\n",
        "         if (u < 0 || u >= num_nodes) return;\n",
        "         int start_edge = indptr[u];\n",
        "         int end_edge = indptr[u + 1];\n",
        "         for (int i = start_edge; i < end_edge; ++i) {\n",
        "             int v = indices[i];\n",
        "             if (v < 0 || v >= num_nodes) continue;\n",
        "             int old_dist = atomicCAS(&dist[v], -1, current_level + 1);\n",
        "             if (old_dist == -1) {\n",
        "                 int index = atomicAdd(&frontier_size_out[1], 1);\n",
        "                 frontier_out[index] = v;\n",
        "             }\n",
        "         }\n",
        "    }\n",
        "}\n",
        "'''\n",
        "bfs_module = compiler.SourceModule(bfs_kernel_code_pycuda)\n",
        "bfs_kernel_pycuda = bfs_module.get_function(\"bfs_kernel\")\n",
        "# --- End BFS Code ---\n",
        "\n",
        "\n",
        "# --- Dijkstra Code (No changes needed) ---\n",
        "dijkstra_kernel_code_pycuda = r'''\n",
        "#include <float.h>\n",
        "\n",
        "static __device__ float atomicMinFloat(float* addr, float value) {\n",
        "    float old;\n",
        "    unsigned int* addr_as_int = (unsigned int*)addr;\n",
        "    unsigned int old_int = *addr_as_int;\n",
        "    unsigned int assumed;\n",
        "    unsigned int value_as_int = __float_as_int(value);\n",
        "    do {\n",
        "        assumed = old_int;\n",
        "        old = __int_as_float(old_int);\n",
        "        if (value >= old) { break; }\n",
        "        old_int = atomicCAS(addr_as_int, assumed, value_as_int);\n",
        "    } while (assumed != old_int);\n",
        "    return old;\n",
        "}\n",
        "\n",
        "extern \"C\" __global__\n",
        "void dijkstra_relax_kernel(const int* indptr, const int* indices, const float* weights,\n",
        "                           float* dist, const bool* active_nodes_in, bool* active_nodes_out,\n",
        "                           bool* changed, int num_nodes) {\n",
        "    int u = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (u >= num_nodes) return;\n",
        "    bool is_active = active_nodes_in[u];\n",
        "    active_nodes_out[u] = is_active;\n",
        "    if (!is_active) { return; }\n",
        "    active_nodes_out[u] = false;\n",
        "    float u_dist = dist[u];\n",
        "    if (u_dist == FLT_MAX) return;\n",
        "    int start_edge = indptr[u];\n",
        "    int end_edge = indptr[u + 1];\n",
        "    for (int i = start_edge; i < end_edge; ++i) {\n",
        "        int v = indices[i];\n",
        "        float weight = weights[i];\n",
        "        if (v < 0 || v >= num_nodes) continue;\n",
        "        float new_dist_v = u_dist + weight;\n",
        "        float old_dist_v = atomicMinFloat(&dist[v], new_dist_v);\n",
        "        if (new_dist_v < old_dist_v) {\n",
        "             active_nodes_out[v] = true;\n",
        "             atomicCAS((unsigned int*)&changed[0], 0, 1);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "'''\n",
        "dijkstra_module = compiler.SourceModule(dijkstra_kernel_code_pycuda)\n",
        "dijkstra_relax_kernel_pycuda = dijkstra_module.get_function(\"dijkstra_relax_kernel\")\n",
        "# --- End Dijkstra Code ---\n",
        "\n",
        "\n",
        "# --- PageRank Kernels (CORRECTED) ---\n",
        "pagerank_kernels_pycuda_corrected = r'''\n",
        "// Kernel 1: Calculate Contributions & Dangling Sum (No change)\n",
        "extern \"C\" __global__\n",
        "void pagerank_kernel(const int* indptr, const int* indices, const float* ranks_in, float* ranks_out,\n",
        "                      const int* out_degree, float* dangling_sum,\n",
        "                      int num_nodes) {\n",
        "    int u = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (u >= num_nodes) return;\n",
        "    float rank_u = ranks_in[u];\n",
        "    int degree_u = out_degree[u];\n",
        "    if (degree_u > 0) {\n",
        "        float contribution = rank_u / degree_u;\n",
        "        int start_edge = indptr[u];\n",
        "        int end_edge = indptr[u + 1];\n",
        "        for (int i = start_edge; i < end_edge; ++i) {\n",
        "            int v = indices[i];\n",
        "            if (v >= 0 && v < num_nodes) {\n",
        "                 atomicAdd(&ranks_out[v], contribution);\n",
        "            }\n",
        "        }\n",
        "    } else {\n",
        "        atomicAdd(&dangling_sum[0], rank_u);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel 2: Apply Damping Factor & Dangling Contribution (No change)\n",
        "extern \"C\" __global__\n",
        "void pagerank_finish_kernel(float* ranks_out, const float* dangling_sum,\n",
        "                             int num_nodes, float damping_factor, float base_rank) {\n",
        "    int v = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (v >= num_nodes) return;\n",
        "    float total_dangling_rank = dangling_sum[0];\n",
        "    float dangling_contribution = (num_nodes > 0) ? (total_dangling_rank / num_nodes) : 0.0f;\n",
        "    ranks_out[v] = base_rank + damping_factor * (ranks_out[v] + dangling_contribution);\n",
        "}\n",
        "\n",
        "\n",
        "// Kernel 3: Reduction for Convergence Check (CORRECTED)\n",
        "extern \"C\" __global__\n",
        "void reduce_diff_kernel(const float* arr1, const float* arr2, float* result, int n) {\n",
        "    // Use dynamically sized shared memory, declared via 'shared=' parameter in kernel launch\n",
        "    extern __shared__ float sdata[];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; // Global index\n",
        "    unsigned int gridSize = gridDim.x * blockDim.x; // Total number of threads in grid\n",
        "\n",
        "    float my_sum = 0.0f;\n",
        "\n",
        "    // Each thread computes sum for multiple elements if n > gridSize (grid-stride loop)\n",
        "    while (i < n) {\n",
        "        my_sum += fabsf(arr1[i] - arr2[i]);\n",
        "        i += gridSize;\n",
        "    }\n",
        "    sdata[tid] = my_sum; // Store thread's partial sum into shared memory\n",
        "\n",
        "    __syncthreads(); // Wait for all threads in block to finish calculating and storing\n",
        "\n",
        "    // Perform reduction in shared memory\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads(); // Wait for partial sum to complete at each step\n",
        "    }\n",
        "\n",
        "    // Block leader (thread 0) adds this block's total sum to the global result atomically\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(result, sdata[0]);\n",
        "    }\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile the corrected PageRank kernels\n",
        "try:\n",
        "    pagerank_module = compiler.SourceModule(pagerank_kernels_pycuda_corrected)\n",
        "    pagerank_kernel_pycuda = pagerank_module.get_function(\"pagerank_kernel\")\n",
        "    pagerank_finish_kernel_pycuda = pagerank_module.get_function(\"pagerank_finish_kernel\")\n",
        "    reduce_diff_kernel_pycuda = pagerank_module.get_function(\"reduce_diff_kernel\")\n",
        "    print(\"PageRank kernels compiled successfully.\")\n",
        "except compiler.CompileError as e:\n",
        "    print(\"ERROR compiling PageRank kernels:\")\n",
        "    print(e)\n",
        "    # Handle error appropriately, maybe exit or skip PageRank\n",
        "\n",
        "\n",
        "# --- Host Code (run_..._gpu_timed_pycuda functions) ---\n",
        "# Make sure the Python functions run_bfs_gpu_timed_pycuda, run_dijkstra_gpu_timed_pycuda,\n",
        "# and run_pagerank_gpu_timed_pycuda defined in the previous PyCUDA block are present.\n",
        "# The run_pagerank_gpu_timed_pycuda function already correctly calculates\n",
        "# shared_mem_bytes and passes it via the 'shared=' argument when launching\n",
        "# reduce_diff_kernel_pycuda, so no changes are needed in the host code for that part.\n",
        "\n",
        "# --- Re-paste host code if needed ---\n",
        "def run_bfs_gpu_timed_pycuda(indptr_cpu, indices_cpu, num_nodes, start_node=0):\n",
        "    print(f\"\\nRunning BFS from node {start_node} on GPU (PyCUDA)...\")\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes)\n",
        "    d_dist = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier1 = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier2 = drv.mem_alloc(num_nodes * np.int32().itemsize)\n",
        "    d_frontier_size = drv.mem_alloc(2 * np.int32().itemsize)\n",
        "    drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    drv.memcpy_htod(d_indices, indices_cpu)\n",
        "    dist_init_cpu = np.full(num_nodes, -1, dtype=np.int32)\n",
        "    drv.memcpy_htod(d_dist, dist_init_cpu)\n",
        "    frontier_size_init_cpu = np.zeros(2, dtype=np.int32)\n",
        "    drv.memcpy_htod(d_frontier_size, frontier_size_init_cpu)\n",
        "    dist_init_cpu[start_node] = 0\n",
        "    drv.memcpy_htod(d_dist, dist_init_cpu)\n",
        "    frontier1_init_cpu = np.zeros(num_nodes, dtype=np.int32)\n",
        "    frontier1_init_cpu[0] = start_node\n",
        "    drv.memcpy_htod(d_frontier1, frontier1_init_cpu)\n",
        "    frontier_size_init_cpu[0] = 1\n",
        "    drv.memcpy_htod(d_frontier_size, frontier_size_init_cpu)\n",
        "    current_level = 0\n",
        "    threads_per_block = 256\n",
        "    frontier_in_gpu = d_frontier1\n",
        "    frontier_out_gpu = d_frontier2\n",
        "    start_event = drv.Event()\n",
        "    end_event = drv.Event()\n",
        "    frontier_size_h = np.zeros(2, dtype=np.int32)\n",
        "    frontier_size_h_reset_next = np.zeros(2, dtype=np.int32)\n",
        "    start_event.record()\n",
        "    while True:\n",
        "        drv.memcpy_dtoh(frontier_size_h, d_frontier_size)\n",
        "        current_frontier_size = int(frontier_size_h[0])\n",
        "        if current_frontier_size == 0: break\n",
        "        frontier_size_h_reset_next[0] = current_frontier_size\n",
        "        frontier_size_h_reset_next[1] = 0\n",
        "        drv.memcpy_htod(d_frontier_size, frontier_size_h_reset_next)\n",
        "        grid_dim, block_dim = get_kernel_config_pycuda(current_frontier_size, threads_per_block)\n",
        "        args = [d_indptr, d_indices, d_dist, frontier_in_gpu, frontier_out_gpu,\n",
        "                d_frontier_size, np.int32(current_level), np.int32(num_nodes)]\n",
        "        bfs_kernel_pycuda(*args, block=block_dim, grid=grid_dim)\n",
        "        drv.memcpy_dtoh(frontier_size_h, d_frontier_size)\n",
        "        next_frontier_size = int(frontier_size_h[1])\n",
        "        frontier_size_h[0] = next_frontier_size\n",
        "        drv.memcpy_htod(d_frontier_size, frontier_size_h)\n",
        "        frontier_in_gpu, frontier_out_gpu = frontier_out_gpu, frontier_in_gpu\n",
        "        current_level += 1\n",
        "    end_event.record()\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0\n",
        "    dist_result_cpu = np.empty_like(dist_init_cpu)\n",
        "    drv.memcpy_dtoh(dist_result_cpu, d_dist)\n",
        "    d_indptr.free(); d_indices.free(); d_dist.free(); d_frontier1.free(); d_frontier2.free(); d_frontier_size.free()\n",
        "    print(f\"GPU BFS (PyCUDA) finished in {gpu_time:.4f} seconds.\")\n",
        "    unvisited_count = np.sum(dist_result_cpu == -1)\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {num_nodes - unvisited_count}\")\n",
        "    return dist_result_cpu, gpu_time\n",
        "\n",
        "def run_dijkstra_gpu_timed_pycuda(indptr_cpu, indices_cpu, weights_cpu, num_nodes, start_node=0, max_iterations=1000):\n",
        "    print(f\"\\nRunning Dijkstra from node {start_node} on GPU (PyCUDA)...\")\n",
        "    infinity = np.finfo(np.float32).max\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes); drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes); drv.memcpy_htod(d_indices, indices_cpu)\n",
        "    d_weights = drv.mem_alloc(weights_cpu.nbytes); drv.memcpy_htod(d_weights, weights_cpu)\n",
        "    d_dist = drv.mem_alloc(num_nodes * np.float32().itemsize)\n",
        "    bool_itemsize = np.dtype(np.bool_).itemsize\n",
        "    d_active1 = drv.mem_alloc(num_nodes * bool_itemsize)\n",
        "    d_active2 = drv.mem_alloc(num_nodes * bool_itemsize)\n",
        "    d_changed = drv.mem_alloc(1 * bool_itemsize)\n",
        "    dist_cpu = np.full(num_nodes, infinity, dtype=np.float32)\n",
        "    active1_cpu = np.zeros(num_nodes, dtype=np.bool_)\n",
        "    changed_cpu = np.zeros(1, dtype=np.bool_)\n",
        "    dist_cpu[start_node] = 0.0; active1_cpu[start_node] = True\n",
        "    drv.memcpy_htod(d_dist, dist_cpu)\n",
        "    drv.memcpy_htod(d_active1, active1_cpu)\n",
        "    active_in_gpu = d_active1; active_out_gpu = d_active2\n",
        "    threads_per_block = 256\n",
        "    grid_dim, block_dim = get_kernel_config_pycuda(num_nodes, threads_per_block)\n",
        "    start_event = drv.Event(); end_event = drv.Event()\n",
        "    iterations_done = 0\n",
        "    start_event.record()\n",
        "    for iteration in range(max_iterations):\n",
        "        iterations_done = iteration + 1\n",
        "        changed_cpu[0] = False; drv.memcpy_htod(d_changed, changed_cpu)\n",
        "        args = [d_indptr, d_indices, d_weights, d_dist,\n",
        "                active_in_gpu, active_out_gpu, d_changed, np.int32(num_nodes)]\n",
        "        dijkstra_relax_kernel_pycuda(*args, block=block_dim, grid=grid_dim)\n",
        "        drv.memcpy_dtoh(changed_cpu, d_changed)\n",
        "        if not changed_cpu[0]: print(f\"Convergence reached after {iteration + 1} iterations.\"); break\n",
        "        active_in_gpu, active_out_gpu = active_out_gpu, active_in_gpu\n",
        "    else: print(f\"Warning: Max iterations ({max_iterations}) reached without full convergence.\")\n",
        "    end_event.record()\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0\n",
        "    dist_result_cpu = np.empty_like(dist_cpu)\n",
        "    drv.memcpy_dtoh(dist_result_cpu, d_dist)\n",
        "    d_indptr.free(); d_indices.free(); d_weights.free(); d_dist.free(); d_active1.free(); d_active2.free(); d_changed.free()\n",
        "    print(f\"GPU Dijkstra (PyCUDA) finished in {gpu_time:.4f} seconds after {iterations_done} iterations.\")\n",
        "    reachable_count = np.sum(dist_result_cpu != infinity)\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {reachable_count}\")\n",
        "    if iterations_done == max_iterations and changed_cpu[0]: print(f\"Warning: Max iterations ({max_iterations}) reached without full convergence.\")\n",
        "    return dist_result_cpu, gpu_time\n",
        "\n",
        "\n",
        "def run_pagerank_gpu_timed_pycuda(indptr_cpu, indices_cpu, num_nodes, damping_factor=0.85, max_iterations=100, tolerance=1e-5):\n",
        "    print(f\"\\nRunning PageRank (d={damping_factor}, tol={tolerance}, max_iter={max_iterations}) on GPU (PyCUDA)...\")\n",
        "    out_degree_cpu = np.zeros(num_nodes, dtype=np.int32)\n",
        "    for i in range(num_nodes): out_degree_cpu[i] = indptr_cpu[i+1] - indptr_cpu[i]\n",
        "    d_indptr = drv.mem_alloc(indptr_cpu.nbytes); drv.memcpy_htod(d_indptr, indptr_cpu)\n",
        "    d_indices = drv.mem_alloc(indices_cpu.nbytes); drv.memcpy_htod(d_indices, indices_cpu)\n",
        "    d_out_degree = drv.mem_alloc(out_degree_cpu.nbytes); drv.memcpy_htod(d_out_degree, out_degree_cpu)\n",
        "    d_ranks1 = drv.mem_alloc(num_nodes * np.float32().itemsize)\n",
        "    d_ranks2 = drv.mem_alloc(num_nodes * np.float32().itemsize)\n",
        "    d_dangling_sum = drv.mem_alloc(1 * np.float32().itemsize)\n",
        "    d_diff_sum = drv.mem_alloc(1 * np.float32().itemsize)\n",
        "    ranks_init_cpu = np.full(num_nodes, 1.0 / num_nodes, dtype=np.float32)\n",
        "    drv.memcpy_htod(d_ranks1, ranks_init_cpu)\n",
        "    drv.memset_d32(d_ranks2, 0, num_nodes)\n",
        "    drv.memset_d32(d_dangling_sum, 0, 1)\n",
        "    drv.memset_d32(d_diff_sum, 0, 1)\n",
        "    ranks_in_gpu = d_ranks1; ranks_out_gpu = d_ranks2\n",
        "    base_rank = np.float32((1.0 - damping_factor) / num_nodes)\n",
        "    threads_per_block = 256\n",
        "    grid_dim, block_dim = get_kernel_config_pycuda(num_nodes, threads_per_block)\n",
        "    shared_mem_bytes = threads_per_block * np.float32().itemsize\n",
        "    start_event = drv.Event(); end_event = drv.Event()\n",
        "    iterations_done = 0\n",
        "    diff_val = np.finfo(np.float32).max\n",
        "    start_event.record()\n",
        "    for iteration in range(max_iterations):\n",
        "        iterations_done = iteration + 1\n",
        "        drv.memset_d32(ranks_out_gpu, 0, num_nodes)\n",
        "        drv.memset_d32(d_dangling_sum, 0, 1)\n",
        "        args_k1 = [d_indptr, d_indices, ranks_in_gpu, ranks_out_gpu, d_out_degree, d_dangling_sum, np.int32(num_nodes)]\n",
        "        pagerank_kernel_pycuda(*args_k1, block=block_dim, grid=grid_dim)\n",
        "        args_k2 = [ranks_out_gpu, d_dangling_sum, np.int32(num_nodes), np.float32(damping_factor), base_rank]\n",
        "        pagerank_finish_kernel_pycuda(*args_k2, block=block_dim, grid=grid_dim)\n",
        "        drv.memset_d32(d_diff_sum, 0, 1)\n",
        "        args_k3 = [ranks_out_gpu, ranks_in_gpu, d_diff_sum, np.int32(num_nodes)]\n",
        "        reduce_diff_kernel_pycuda(*args_k3, block=block_dim, grid=grid_dim, shared=shared_mem_bytes)\n",
        "        diff_sum_h = np.zeros(1, dtype=np.float32)\n",
        "        drv.memcpy_dtoh(diff_sum_h, d_diff_sum)\n",
        "        diff_val = diff_sum_h[0]\n",
        "        #if iteration % 10 == 0 or iteration == max_iterations - 1 or diff_val < tolerance: print(f\"  Iteration {iteration+1}: Change = {diff_val:.6f}\")\n",
        "        if diff_val < tolerance: print(f\"Convergence reached after {iteration + 1} iterations.\"); ranks_in_gpu = ranks_out_gpu; break\n",
        "        ranks_in_gpu, ranks_out_gpu = ranks_out_gpu, ranks_in_gpu\n",
        "    else: print(f\"Warning: Max iterations ({max_iterations}) reached without convergence.\")\n",
        "    end_event.record()\n",
        "    end_event.synchronize()\n",
        "    gpu_time = start_event.time_till(end_event) / 1000.0\n",
        "    ranks_result_cpu = np.empty_like(ranks_init_cpu)\n",
        "    drv.memcpy_dtoh(ranks_result_cpu, ranks_in_gpu)\n",
        "    d_indptr.free(); d_indices.free(); d_out_degree.free(); d_ranks1.free(); d_ranks2.free(); d_dangling_sum.free(); d_diff_sum.free()\n",
        "    print(f\"GPU PageRank (PyCUDA) finished in {gpu_time:.4f} seconds after {iterations_done} iterations.\")\n",
        "    print(f\"Sum of ranks: {np.sum(ranks_result_cpu):.4f}\")\n",
        "    if iterations_done == max_iterations and diff_val >= tolerance: print(f\"Warning: Max iterations ({max_iterations}) reached without convergence (last change: {diff_val:.6f}).\")\n",
        "    return ranks_result_cpu, gpu_time\n",
        "\n",
        "# --- End Host Code ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Pxixa564q5",
        "outputId": "92408175-2032-46b5-83cf-dec0eaca5935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n",
            "/usr/local/lib/python3.11/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank kernels compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. CPU Implementations\n",
        "\n",
        "\n",
        "\n",
        "import collections\n",
        "import heapq\n",
        "\n",
        "def run_bfs_cpu(indptr, indices, num_nodes, start_node=0):\n",
        "    print(f\"\\nRunning BFS from node {start_node} on CPU...\")\n",
        "    start_time = time.time()\n",
        "    dist = np.full(num_nodes, -1, dtype=np.int32)\n",
        "    if start_node < 0 or start_node >= num_nodes: return dist\n",
        "    dist[start_node] = 0\n",
        "    queue = collections.deque([start_node])\n",
        "    visited_count = 1\n",
        "    while queue:\n",
        "        u = queue.popleft()\n",
        "        current_level = dist[u]\n",
        "        start_edge, end_edge = indptr[u], indptr[u + 1]\n",
        "        for i in range(start_edge, end_edge):\n",
        "            v = indices[i]\n",
        "            if v >= 0 and v < num_nodes and dist[v] == -1:\n",
        "                dist[v] = current_level + 1\n",
        "                queue.append(v)\n",
        "                visited_count +=1\n",
        "    end_time = time.time()\n",
        "    print(f\"CPU BFS finished in {end_time - start_time:.4f} seconds.\")\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {visited_count}\")\n",
        "    return dist\n",
        "\n",
        "def run_dijkstra_cpu(indptr, indices, weights, num_nodes, start_node=0):\n",
        "    print(f\"\\nRunning Dijkstra from node {start_node} on CPU...\")\n",
        "    start_time = time.time()\n",
        "    infinity = np.finfo(np.float32).max\n",
        "    dist = np.full(num_nodes, infinity, dtype=np.float32)\n",
        "    visited = np.zeros(num_nodes, dtype=bool)\n",
        "    if start_node < 0 or start_node >= num_nodes: return dist\n",
        "    dist[start_node] = 0.0\n",
        "    pq = [(0.0, start_node)]\n",
        "    nodes_processed = 0\n",
        "    while pq:\n",
        "        d_u, u = heapq.heappop(pq)\n",
        "        if d_u > dist[u] or visited[u]: continue\n",
        "        visited[u] = True\n",
        "        nodes_processed += 1\n",
        "        start_edge, end_edge = indptr[u], indptr[u + 1]\n",
        "        for i in range(start_edge, end_edge):\n",
        "            v = indices[i]\n",
        "            weight = weights[i]\n",
        "            if v >= 0 and v < num_nodes and not visited[v]:\n",
        "                new_dist_v = dist[u] + weight\n",
        "                if new_dist_v < dist[v]:\n",
        "                    dist[v] = new_dist_v\n",
        "                    heapq.heappush(pq, (new_dist_v, v))\n",
        "    end_time = time.time()\n",
        "    print(f\"CPU Dijkstra finished in {end_time - start_time:.4f} seconds.\")\n",
        "    reachable_count = np.sum(dist != infinity)\n",
        "    print(f\"Number of nodes reachable from start node {start_node}: {reachable_count}\")\n",
        "    return dist\n",
        "\n",
        "def run_pagerank_cpu(indptr, indices, num_nodes, damping_factor=0.85, max_iterations=100, tolerance=1e-5):\n",
        "    print(f\"\\nRunning PageRank (d={damping_factor}, tol={tolerance}, max_iter={max_iterations}) on CPU...\")\n",
        "    start_time = time.time()\n",
        "    out_degree = np.zeros(num_nodes, dtype=np.int32)\n",
        "    for i in range(num_nodes): out_degree[i] = indptr[i+1] - indptr[i]\n",
        "    dangling_nodes = np.where(out_degree == 0)[0]\n",
        "    ranks = np.full(num_nodes, 1.0 / num_nodes, dtype=np.float32)\n",
        "    next_ranks = np.empty(num_nodes, dtype=np.float32)\n",
        "    base_rank = (1.0 - damping_factor) / num_nodes\n",
        "    for iteration in range(max_iterations):\n",
        "        next_ranks.fill(0.0)\n",
        "        dangling_sum = np.sum(ranks[dangling_nodes])\n",
        "        dangling_contribution = dangling_sum / num_nodes\n",
        "        for u in range(num_nodes):\n",
        "            if out_degree[u] > 0:\n",
        "                contribution = ranks[u] / out_degree[u]\n",
        "                start_edge, end_edge = indptr[u], indptr[u+1]\n",
        "                for i in range(start_edge, end_edge):\n",
        "                    v = indices[i]\n",
        "                    if v >= 0 and v < num_nodes: next_ranks[v] += contribution\n",
        "        next_ranks = base_rank + damping_factor * (next_ranks + dangling_contribution)\n",
        "        diff = np.sum(np.abs(next_ranks - ranks))\n",
        "        ranks, next_ranks = next_ranks, ranks\n",
        "        # if iteration % 10 == 0 or iteration == max_iterations - 1: print(f\"  Iteration {iteration+1}: Change = {diff:.6f}\") # Less verbose\n",
        "        if diff < tolerance:\n",
        "            print(f\"Convergence reached after {iteration + 1} iterations.\")\n",
        "            break\n",
        "    else: print(f\"Warning: Max iterations ({max_iterations}) reached without convergence.\")\n",
        "    end_time = time.time()\n",
        "    print(f\"CPU PageRank finished in {end_time - start_time:.4f} seconds.\")\n",
        "    print(f\"Sum of ranks: {np.sum(ranks):.4f}\")\n",
        "    return ranks"
      ],
      "metadata": {
        "id": "m2nse5Ky7f48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Comparison: CPU vs GPU (PyCUDA)\n",
        "\n",
        "print(\"\\n--- Performance Comparison (PyCUDA) ---\")\n",
        "\n",
        "start_node = 0 # Use the same start node\n",
        "\n",
        "# --- BFS Comparison ---\n",
        "print(\"\\n--- BFS ---\")\n",
        "cpu_bfs_start_time = time.time()\n",
        "bfs_dist_cpu = run_bfs_cpu(csr_indptr_cpu, csr_indices_cpu, N, start_node=start_node)\n",
        "cpu_bfs_end_time = time.time()\n",
        "cpu_bfs_time = cpu_bfs_end_time - cpu_bfs_start_time\n",
        "\n",
        "# Run PyCUDA BFS\n",
        "bfs_dist_gpu, gpu_bfs_time_accurate = run_bfs_gpu_timed_pycuda(csr_indptr_cpu, csr_indices_cpu, N, start_node=start_node)\n",
        "\n",
        "print(\"\\nChecking BFS Correctness...\")\n",
        "if np.array_equal(bfs_dist_cpu, bfs_dist_gpu):\n",
        "    print(\"BFS CPU and GPU (PyCUDA) results MATCH.\")\n",
        "else:\n",
        "    print(\"ERROR: BFS CPU and GPU (PyCUDA) results DO NOT MATCH.\")\n",
        "\n",
        "print(f\"BFS CPU Time: {cpu_bfs_time:.4f} s\")\n",
        "print(f\"BFS GPU Time (PyCUDA): {gpu_bfs_time_accurate:.4f} s\")\n",
        "if gpu_bfs_time_accurate > 0:\n",
        "    print(f\"BFS Speedup (CPU/GPU): {cpu_bfs_time / gpu_bfs_time_accurate:.2f}x\")\n",
        "else: print(\"BFS Speedup: GPU time was zero or negative.\")\n",
        "\n",
        "\n",
        "# --- Dijkstra Comparison ---\n",
        "print(\"\\n--- Dijkstra ---\")\n",
        "cpu_dijkstra_start_time = time.time()\n",
        "dijkstra_dist_cpu = run_dijkstra_cpu(csr_indptr_cpu, csr_indices_cpu, csr_weights_cpu, N, start_node=start_node)\n",
        "cpu_dijkstra_end_time = time.time()\n",
        "cpu_dijkstra_time = cpu_dijkstra_end_time - cpu_dijkstra_start_time\n",
        "\n",
        "# Run PyCUDA Dijkstra\n",
        "dijkstra_dist_gpu, gpu_dijkstra_time_accurate = run_dijkstra_gpu_timed_pycuda(csr_indptr_cpu, csr_indices_cpu, csr_weights_cpu, N, start_node=start_node)\n",
        "\n",
        "print(\"\\nChecking Dijkstra Correctness...\")\n",
        "if np.allclose(dijkstra_dist_cpu, dijkstra_dist_gpu, atol=1e-5, rtol=1e-5):\n",
        "    print(\"Dijkstra CPU and GPU (PyCUDA) results MATCH (within tolerance).\")\n",
        "else:\n",
        "    print(\"ERROR: Dijkstra CPU and GPU (PyCUDA) results DO NOT MATCH.\")\n",
        "\n",
        "print(f\"Dijkstra CPU Time: {cpu_dijkstra_time:.4f} s\")\n",
        "print(f\"Dijkstra GPU Time (PyCUDA): {gpu_dijkstra_time_accurate:.4f} s\")\n",
        "if gpu_dijkstra_time_accurate > 0:\n",
        "    print(f\"Dijkstra Speedup (CPU/GPU): {cpu_dijkstra_time / gpu_dijkstra_time_accurate:.2f}x\")\n",
        "else: print(\"Dijkstra Speedup: GPU time was zero or negative.\")\n",
        "\n",
        "\n",
        "# --- PageRank Comparison ---\n",
        "print(\"\\n--- PageRank ---\")\n",
        "cpu_pagerank_start_time = time.time()\n",
        "pagerank_ranks_cpu = run_pagerank_cpu(csr_indptr_cpu, csr_indices_cpu, N)\n",
        "cpu_pagerank_end_time = time.time()\n",
        "cpu_pagerank_time = cpu_pagerank_end_time - cpu_pagerank_start_time\n",
        "\n",
        "# Run PyCUDA PageRank\n",
        "pagerank_ranks_gpu, gpu_pagerank_time_accurate = run_pagerank_gpu_timed_pycuda(csr_indptr_cpu, csr_indices_cpu, N)\n",
        "\n",
        "print(\"\\nChecking PageRank Correctness...\")\n",
        "if np.allclose(pagerank_ranks_cpu, pagerank_ranks_gpu, atol=1e-5, rtol=1e-5):\n",
        "    print(\"PageRank CPU and GPU (PyCUDA) results MATCH (within tolerance).\")\n",
        "else:\n",
        "    print(\"ERROR: PageRank CPU and GPU (PyCUDA) results DO NOT MATCH.\")\n",
        "\n",
        "\n",
        "print(f\"PageRank CPU Time: {cpu_pagerank_time:.4f} s\")\n",
        "print(f\"PageRank GPU Time (PyCUDA): {gpu_pagerank_time_accurate:.4f} s\")\n",
        "if gpu_pagerank_time_accurate > 0:\n",
        "    print(f\"PageRank Speedup (CPU/GPU): {cpu_pagerank_time / gpu_pagerank_time_accurate:.2f}x\")\n",
        "else: print(\"PageRank Speedup: GPU time was zero or negative.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Comparison Summary (PyCUDA) ---\")\n",
        "print(f\"Graph Size: {N} nodes, ~{csr_indices_cpu.shape[0]} edges\")\n",
        "print(f\"Algorithm | CPU Time (s) | GPU Time (s) | Speedup (CPU/GPU)\")\n",
        "print(f\"----------|--------------|--------------|--------------------\")\n",
        "if gpu_bfs_time_accurate > 0: print(f\"BFS       | {cpu_bfs_time:>12.4f} | {gpu_bfs_time_accurate:>12.4f} | {cpu_bfs_time / gpu_bfs_time_accurate:>18.2f}x\")\n",
        "else: print(f\"BFS       | {cpu_bfs_time:>12.4f} | {gpu_bfs_time_accurate:>12.4f} | {'N/A':>18}\")\n",
        "if gpu_dijkstra_time_accurate > 0: print(f\"Dijkstra  | {cpu_dijkstra_time:>12.4f} | {gpu_dijkstra_time_accurate:>12.4f} | {cpu_dijkstra_time / gpu_dijkstra_time_accurate:>18.2f}x\")\n",
        "else: print(f\"Dijkstra  | {cpu_dijkstra_time:>12.4f} | {gpu_dijkstra_time_accurate:>12.4f} | {'N/A':>18}\")\n",
        "if gpu_pagerank_time_accurate > 0: print(f\"PageRank  | {cpu_pagerank_time:>12.4f} | {gpu_pagerank_time_accurate:>12.4f} | {cpu_pagerank_time / gpu_pagerank_time_accurate:>18.2f}x\")\n",
        "else: print(f\"PageRank  | {cpu_pagerank_time:>12.4f} | {gpu_pagerank_time_accurate:>12.4f} | {'N/A':>18}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROBB34rJ7nDy",
        "outputId": "4b193f6b-d47d-44e5-8d5a-261485573da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performance Comparison (PyCUDA) ---\n",
            "\n",
            "--- BFS ---\n",
            "\n",
            "Running BFS from node 0 on CPU...\n",
            "CPU BFS finished in 0.0533 seconds.\n",
            "Number of nodes reachable from start node 0: 4096\n",
            "\n",
            "Running BFS from node 0 on GPU (PyCUDA)...\n",
            "GPU BFS (PyCUDA) finished in 0.0013 seconds.\n",
            "Number of nodes reachable from start node 0: 4096\n",
            "\n",
            "Checking BFS Correctness...\n",
            "BFS CPU and GPU (PyCUDA) results MATCH.\n",
            "BFS CPU Time: 0.0535 s\n",
            "BFS GPU Time (PyCUDA): 0.0013 s\n",
            "BFS Speedup (CPU/GPU): 41.79x\n",
            "\n",
            "--- Dijkstra ---\n",
            "\n",
            "Running Dijkstra from node 0 on CPU...\n",
            "CPU Dijkstra finished in 0.0884 seconds.\n",
            "Number of nodes reachable from start node 0: 4096\n",
            "\n",
            "Running Dijkstra from node 0 on GPU (PyCUDA)...\n",
            "Convergence reached after 8 iterations.\n",
            "GPU Dijkstra (PyCUDA) finished in 0.0007 seconds after 8 iterations.\n",
            "Number of nodes reachable from start node 0: 4096\n",
            "\n",
            "Checking Dijkstra Correctness...\n",
            "Dijkstra CPU and GPU (PyCUDA) results MATCH (within tolerance).\n",
            "Dijkstra CPU Time: 0.0886 s\n",
            "Dijkstra GPU Time (PyCUDA): 0.0007 s\n",
            "Dijkstra Speedup (CPU/GPU): 120.55x\n",
            "\n",
            "--- PageRank ---\n",
            "\n",
            "Running PageRank (d=0.85, tol=1e-05, max_iter=100) on CPU...\n",
            "Convergence reached after 8 iterations.\n",
            "CPU PageRank finished in 1.0160 seconds.\n",
            "Sum of ranks: 1.0000\n",
            "\n",
            "Running PageRank (d=0.85, tol=1e-05, max_iter=100) on GPU (PyCUDA)...\n",
            "Convergence reached after 8 iterations.\n",
            "GPU PageRank (PyCUDA) finished in 0.0007 seconds after 8 iterations.\n",
            "Sum of ranks: 1.0000\n",
            "\n",
            "Checking PageRank Correctness...\n",
            "PageRank CPU and GPU (PyCUDA) results MATCH (within tolerance).\n",
            "PageRank CPU Time: 1.0162 s\n",
            "PageRank GPU Time (PyCUDA): 0.0007 s\n",
            "PageRank Speedup (CPU/GPU): 1455.90x\n",
            "\n",
            "--- Comparison Summary (PyCUDA) ---\n",
            "Graph Size: 4096 nodes, ~131072 edges\n",
            "Algorithm | CPU Time (s) | GPU Time (s) | Speedup (CPU/GPU)\n",
            "----------|--------------|--------------|--------------------\n",
            "BFS       |       0.0535 |       0.0013 |              41.79x\n",
            "Dijkstra  |       0.0886 |       0.0007 |             120.55x\n",
            "PageRank  |       1.0162 |       0.0007 |            1455.90x\n"
          ]
        }
      ]
    }
  ]
}